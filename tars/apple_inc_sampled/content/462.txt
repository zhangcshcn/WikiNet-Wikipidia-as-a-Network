

  In
  
   computing
  
  ,
  
   scheduling
  
  is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as
  
   threads
  
  ,
  
   processes
  
  or data
  
   flows
  
  , which are in turn scheduled onto hardware resources such as
  
   processors
  
  ,
  
   network links
  
  or
  
   expansion cards
  
  .
 
  A scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all computer resources busy (as in
  
   load balancing
  
  ), allow multiple users to share system resources effectively, or to achieve a target
  
   quality of service
  
  . Scheduling is fundamental to computation itself, and an intrinsic part of the
  
   execution model
  
  of a computer system; the concept of scheduling makes it possible to have
  
   computer multitasking
  
  with a single
  
   central processing unit
  
  (CPU).
 
  A scheduler may aim at one of many goals, for example, maximizing
  

    throughput
   

  (the total amount of work completed per time unit), minimizing
  

    response time
   

  (time from work becoming enabled until the first point it begins execution on resources), or minimizing
  

    latency
   

  (the time between work becoming enabled and its subsequent completion),
  

    [1]
   

  maximizing
  
   fairness
  
  (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.
 
  In
  
   real-time
  
  environments, such as
  
   embedded systems
  
  for
  
   automatic control
  
  in industry (for example
  
   robotics
  
  ), the scheduler also must ensure that processes can meet
  
   deadlines
  
  ; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and
  
   managed
  
  through an administrative back end.
 


  The scheduler is an operating system module that selects the next jobs to be admitted into the system and the next process to run. Operating systems may feature up to three distinct scheduler types: a
  
   long-term scheduler
  
  (also known as an admission scheduler or high-level scheduler), a
  
   mid-term or medium-term scheduler
  
  , and a
  
   short-term scheduler
  
  . The names suggest the relative frequency with which their functions are performed.
 
  The process scheduler is a part of the operating system that decides which process runs at a certain point in time. It usually has the ability to pause a running process, move it to the back of the running queue and start a new process; such a scheduler is known as
  

    preemptive
   
   scheduler
  
  , otherwise it is a
  

    cooperative
   
   scheduler
  
  .
  

    [2]
   


  The
  
   long-term scheduler
  
  , or
  
   admission scheduler
  
  , decides which jobs or processes are to be admitted to the ready queue (in main memory); that is, when an attempt is made to execute a program, its admission to the set of currently executing processes is either authorized or delayed by the long-term scheduler. Thus, this scheduler dictates what processes are to run on a system, and the degree of concurrency to be supported at any one time –  whether many or few processes are to be executed concurrently, and how the split between I/O-intensive and CPU-intensive processes is to be handled. The long-term scheduler is responsible for controlling the degree of multiprogramming.
 
  In general, most processes can be described as either
  
   I/O-bound
  
  or
  
   CPU-bound
  
  . An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations. It is important that a long-term scheduler selects a good process mix of I/O-bound and CPU-bound processes. If all processes are I/O-bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. On the other hand, if all processes are CPU-bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes. In modern operating systems, this is used to make sure that real-time processes get enough CPU time to finish their tasks.
  

    [3]
   


  Long-term scheduling is also important in large-scale systems such as
  
   batch processing
  
  systems,
  
   computer clusters
  
  ,
  
   supercomputers
  
  , and
  
   render farms
  
  . For example, in
  
   concurrent systems
  
  ,
  
   coscheduling
  
  of interacting processes is often required to prevent them from blocking due to waiting on each other. In these cases, special-purpose
  
   job scheduler
  
  software is typically used to assist these functions, in addition to any underlying admission scheduling support in the operating system.
 
  The
  
   medium-term scheduler
  
  temporarily removes processes from main memory and places them in secondary memory (such as a
  
   hard disk drive
  
  ) or vice versa, which is commonly referred to as "swapping out" or "swapping in" (also incorrectly as "
  
   paging
  
  out" or "paging in"). The medium-term scheduler may decide to swap out a process which has not been active for some time, or a process which has a low priority, or a process which is
  
   page faulting
  
  frequently, or a process which is taking up a large amount of memory in order to free up main memory for other processes, swapping the process back in later when more memory is available, or when the process has been unblocked and is no longer waiting for a resource. [Stallings, 396] [Stallings, 370]
 
  In many systems today (those that support mapping virtual address space to secondary storage other than the swap file), the medium-term scheduler may actually perform the role of the long-term scheduler, by treating binaries as "swapped out processes" upon their execution. In this way, when a segment of the binary is required it can be swapped in on demand, or "lazy loaded". [Stallings, 394]
 
  The
  
   short-term scheduler
  
  (also known as the
  
   CPU scheduler
  
  ) decides which of the ready, in-memory processes is to be executed (allocated a CPU) after a clock
  
   interrupt
  
  , an I/O interrupt, an operating
  
   system call
  
  or another form of
  
   signal
  
  . Thus the short-term scheduler makes scheduling decisions much more frequently than the long-term or mid-term schedulers – a scheduling decision will at a minimum have to be made after every time slice, and these are very short. This scheduler can be
  
   preemptive
  
  , implying that it is capable of forcibly removing processes from a CPU when it decides to allocate that CPU to another process, or non-preemptive (also known as "voluntary" or "co-operative"), in which case the scheduler is unable to "force" processes off the CPU.
 
  A preemptive scheduler relies upon a
  
   programmable interval timer
  
  which invokes an
  
   interrupt handler
  
  that runs in
  
   kernel mode
  
  and implements the scheduling function.
 
  Another component that is involved in the CPU-scheduling function is the dispatcher, which is the module that gives control of the CPU to the process selected by the short-term scheduler. It receives control in kernel mode as the result of an interrupt or system call. The functions of a dispatcher involve the following:
 
  The dispatcher should be as fast as possible, since it is invoked during every process switch. During the context switches, the processor is virtually idle for a fraction of time, thus unnecessary context switches should be avoided. The time it takes for the dispatcher to stop one process and start another is known as the
  
   dispatch latency
  
  .
  

    [3]
   


   :155
  

  Scheduling disciplines are algorithms used for distributing resources among parties which simultaneously and asynchronously request them. Scheduling disciplines are used in
  
   routers
  
  (to handle packet traffic) as well as in
  
   operating systems
  
  (to share
  
   CPU time
  
  among both
  
   threads
  
  and
  
   processes
  
  ), disk drives (
  
   I/O scheduling
  
  ), printers (
  
   print spooler
  
  ), most embedded systems, etc.
 
  The main purposes of scheduling algorithms are to minimize
  
   resource starvation
  
  and to ensure fairness amongst the parties utilizing the resources. Scheduling deals with the problem of deciding which of the outstanding requests is to be allocated resources. There are many different scheduling algorithms. In this section, we introduce several of them.
 
  In
  
   packet-switched
  

   computer networks
  
  and other
  
   statistical multiplexing
  
  , the notion of a
  
   scheduling algorithm
  
  is used as an alternative to
  
   first-come first-served
  
  queuing of data packets.
 
  The simplest best-effort scheduling algorithms are
  
   round-robin
  
  ,
  
   fair queuing
  
  (a
  
   max-min fair
  
  scheduling algorithm),
  
   proportionally fair
  
  scheduling and
  
   maximum throughput
  
  . If differentiated or guaranteed
  
   quality of service
  
  is offered, as opposed to best-effort communication,
  
   weighted fair queuing
  
  may be utilized.
 
  In advanced packet radio wireless networks such as
  
   HSDPA
  
  (High-Speed Downlink Packet Access )
  
   3.5G
  
  cellular system,
  
   channel-dependent scheduling
  
  may be used to take advantage of
  
   channel state information
  
  . If the channel conditions are favourable, the
  
   throughput
  
  and
  
   system spectral efficiency
  
  may be increased. In even more advanced systems such as
  
   LTE
  
  , the scheduling is combined by channel-dependent packet-by-packet
  
   dynamic channel allocation
  
  , or by assigning
  
   OFDMA
  
  multi-carriers or other
  
   frequency-domain equalization
  
  components to the users that best can utilize them.
  

    [4]
   



   First in, first out
  
  (
  
   FIFO
  
  ), also known as
  
   first come, first served
  
  (FCFS), is the simplest scheduling algorithm. FIFO simply queues processes in the order that they arrive in the ready queue. This is commonly used for a
  

    task queue
   

  , for example as illustrated in this section.
 
  Earliest deadline first (EDF) or
  
   least time to go
  
  is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (a task finishes, new task is released, etc.), the queue will be searched for the process closest to its deadline, which will be the next to be scheduled for execution.
 
  Similar to
  
   shortest job first
  
  (SJF). With this strategy the scheduler arranges processes with the least estimated processing time remaining to be next in the queue. This requires advanced knowledge or estimations about the time required for a process to complete.
 
  The operating system assigns a fixed priority rank to every process, and the scheduler arranges the processes in the ready queue in order of their priority. Lower-priority processes get interrupted by incoming higher-priority processes.
 
  The scheduler assigns a fixed time unit per process, and cycles through them. If process completes within that time-slice it got terminated otherwise it is rescheduled after giving a chance to all other processes.
 
  This is used for situations in which processes are easily divided into different groups. For example, a common division is made between foreground (interactive) processes and background (batch) processes. These two types of processes have different response-time requirements and so may have different scheduling needs. It is very useful for shared memory problems.
 
  A
  
   work-conserving scheduler
  
  is a scheduler that always tries to keep the scheduled resources busy, if there are submitted jobs ready to be scheduled. In contrast, a non-work conserving scheduler is a scheduler that, in some cases, may leave the scheduled resources idle despite the presence of jobs ready to be scheduled.
 
  There are several scheduling problems in which the goal is to decide which job goes to which station at what time, such that the total
  
   makespan
  
  is minimized:
 
  A very common method in embedded systems is to manually schedule jobs. This can for example be done in a time-multiplexed fashion. Sometimes the kernel is divided in three or more parts: Manual scheduling, preemptive and interrupt level. Exact methods for scheduling jobs are often proprietary.
 
  When designing an operating system, a programmer must consider which scheduling algorithm will perform best for the use the system is going to see. There is no universal “best” scheduling algorithm, and many operating systems use extended or combinations of the scheduling algorithms above.
 
  For example,
  
   Windows NT
  
  /XP/Vista uses a
  
   multilevel feedback queue
  
  , a combination of fixed-priority preemptive scheduling, round-robin, and first in, first out algorithms. In this system, threads can dynamically increase or decrease in priority depending on if it has been serviced already, or if it has been waiting extensively. Every priority level is represented by its own queue, with
  
   round-robin scheduling
  
  among the high-priority threads and
  
   FIFO
  
  among the lower-priority ones. In this sense, response time is short for most threads, and short but critical system threads get completed very quickly. Since threads can only use one time unit of the round-robin in the highest-priority queue, starvation can be a problem for longer high-priority threads.
 
  The algorithm used may be as simple as
  
   round-robin
  
  in which each process is given equal time (for instance 1 ms, usually between 1 ms and 100 ms) in a cycling list. So, process A executes for 1 ms, then process B, then process C, then back to process A.
 
  More advanced algorithms take into account process priority, or the importance of the process. This allows some processes to use more time than other processes. The kernel always uses whatever resources it needs to ensure proper functioning of the system, and so can be said to have infinite priority. In
  
   SMP
  
  (symmetric multiprocessing) systems,
  
   processor affinity
  
  is considered to increase overall system performance, even if it may cause a process itself to run more slowly. This generally improves performance by reducing
  
   cache thrashing
  
  .
 
  IBM
  
   OS/360
  
  was available with three different schedulers. The differences were such that the variants were often considered three different operating systems:
 
  Later virtual storage versions of MVS added a
  

    Workload Manager
   

  feature to the scheduler, which schedules processor resources according to an elaborate scheme defined by the installation.
 
  Very early
  
   MS-DOS
  
  and Microsoft Windows systems were non-multitasking, and as such did not feature a scheduler.
  
   Windows 3.1x
  
  used a non-preemptive scheduler, meaning that it did not interrupt programs. It relied on the program to end or tell the OS that it didn't need the processor so that it could move on to another process. This is usually called cooperative multitasking. Windows 95 introduced a rudimentary preemptive scheduler; however, for legacy support opted to let 16 bit applications run without preemption.
  

    [5]
   



   Windows NT
  
  -based operating systems use a multilevel feedback queue. 32 priority levels are defined, 0 through to 31, with priorities 0 through 15 being "normal" priorities and priorities 16 through 31 being soft real-time priorities, requiring privileges to assign. 0 is reserved for the Operating System. Users can select 5 of these priorities to assign to a running application from the Task Manager application, or through thread management APIs. The kernel may change the priority level of a thread depending on its I/O and CPU usage and whether it is interactive (i.e. accepts and responds to input from humans), raising the priority of interactive and I/O bounded processes and lowering that of CPU bound processes, to increase the responsiveness of interactive applications.
  

    [6]
   

  The scheduler was modified in
  
   Windows Vista
  
  to use the
  
   cycle counter register
  
  of modern processors to keep track of exactly how many CPU cycles a thread has executed, rather than just using an interval-timer interrupt routine.
  

    [7]
   

  Vista also uses a priority scheduler for the I/O queue so that disk defragmenters and other such programs do not interfere with foreground operations.
  

    [8]
   


  Mac OS 9 uses cooperative scheduling for threads, where one process controls multiple cooperative threads, and also provides preemptive scheduling for multiprocessing tasks. The kernel schedules multiprocessing tasks using a preemptive scheduling algorithm. All Process Manager processes run within a special multiprocessing task, called the "blue task". Those processes are scheduled cooperatively, using a
  
   round-robin scheduling
  
  algorithm; a process yields control of the processor to another process by explicitly calling a blocking function such as
  
   WaitNextEvent
  
  . Each process has its own copy of the
  
   Thread Manager
  
  that schedules that process's threads cooperatively; a thread yields control of the processor to another thread by calling
  
   YieldToAnyThread
  
  or
  
   YieldToThread
  
  .
  

    [9]
   


  macOS uses a multilevel feedback queue, with four priority bands for threads – normal, system high priority, kernel mode only, and real-time.
  

    [10]
   

  Threads are scheduled preemptively; macOS also supports cooperatively scheduled threads in its implementation of the Thread Manager in
  
   Carbon
  
  .
  

    [9]
   


  In AIX Version 4 there are three possible values for thread scheduling policy:
 
  Threads are primarily of interest for applications that currently consist of several asynchronous processes. These applications might impose a lighter load on the system if converted to a multithreaded structure.
 
  AIX 5 implements the following scheduling policies: FIFO, round robin, and a fair round robin. The FIFO policy has three different implementations: FIFO, FIFO2, and FIFO3. The round robin policy is named SCHED_RR in AIX, and the fair round robin is called SCHED_OTHER.
  

    [11]
   


  In
  
   Linux
  
  2.4, an
  
   O(n) scheduler
  
  with a
  
   multilevel feedback queue
  
  with priority levels ranging from 0 to 140 was used; 0–99 are reserved for real-time tasks and 100–140 are considered
  
   nice
  
  task levels. For real-time tasks, the time quantum for switching processes was approximately 200 ms, and for nice tasks approximately 10 ms.
  
   [
   


      citation needed
     


   ]
  
  The scheduler ran through the
  
   run queue
  
  of all ready processes, letting the highest priority processes go first and run through their time slices, after which they will be placed in an expired queue. When the active queue is empty the expired queue will become the active queue and vice versa.
 
  However, some enterprise
  
   Linux distributions
  
  such as
  
   SUSE Linux Enterprise Server
  
  replaced this scheduler with a backport of the
  
   O(1) scheduler
  
  (which was maintained by
  
   Alan Cox
  
  in his Linux 2.4-ac Kernel series) to the Linux 2.4 kernel used by the distribution.
 
  In versions 2.6.0 to 2.6.22, the kernel used an
  
   O(1) scheduler
  
  developed by
  
   Ingo Molnar
  
  and many other kernel developers during the Linux 2.5 development. For many kernel in time frame,
  
   Con Kolivas
  
  developed patch sets which improved interactivity with this scheduler or even replaced it with his own schedulers.
 
  Con Kolivas's work, most significantly his implementation of "
  
   fair scheduling
  
  " named "Rotating Staircase Deadline", inspired Ingo Molnár to develop the
  
   Completely Fair Scheduler
  
  as a replacement for the earlier
  
   O(1) scheduler
  
  , crediting Kolivas in his announcement.
  

    [12]
   

  CFS is the first implementation of a fair queuing
  
   process scheduler
  
  widely used in a general-purpose operating system.
  

    [13]
   


  The
  
   Completely Fair Scheduler
  
  (CFS) uses a well-studied, classic scheduling algorithm called
  
   fair queuing
  
  originally invented for
  
   packet networks
  
  . Fair queuing had been previously applied to CPU scheduling under the name
  
   stride scheduling
  
  . The fair queuing CFS scheduler has a scheduling complexity of O(
  
   log
  
  N), where N is the number of tasks in the
  
   runqueue
  
  . Choosing a task can be done in constant time, but reinserting a task after it has run requires O(log N) operations, because the
  
   run queue
  
  is implemented as a
  
   red-black tree
  
  .
 
  The
  
   Brain Fuck Scheduler
  
  (BFS), also created by Con Kolivas, is an alternative to the CFS.
 

   FreeBSD
  
  uses a multilevel feedback queue with priorities ranging from 0–255. 0–63 are reserved for interrupts, 64–127 for the top half of the kernel, 128–159 for real-time user threads, 160–223 for time-shared user threads, and 224–255 for idle user threads. Also, like Linux, it uses the active queue setup, but it also has an idle queue.
  

    [14]
   



   NetBSD
  
  uses a multilevel feedback queue with priorities ranging from 0–223. 0–63 are reserved for time-shared threads (default, SCHED_OTHER policy), 64–95 for user threads which entered
  
   kernel space
  
  , 96-128 for kernel threads, 128–191 for user real-time threads (SCHED_FIFO and SCHED_RR policies), and 192–223 for
  
   software interrupts
  
  .
 

   Solaris
  
  uses a multilevel feedback queue with priorities ranging between 0 and 169. Priorities 0–59 are reserved for time-shared threads, 60–99 for system threads, 100–159 for real-time threads, and 160–169 for low priority interrupts. Unlike Linux,
  

    [14]
   

  when a process is done using its time quantum, it is given a new priority and put back in the queue. Solaris 9 introduced two new scheduling classes, namely fixed priority class and fair share class. The threads with fixed priority have the same priority range as that of the time-sharing class, but their priorities are not dynamically adjusted. The fair scheduling class uses CPU
  
   shares
  
  to prioritize threads for scheduling decisions. CPU shares indicate the entitlement to CPU resources. They are allocated to a set of processes, which are collectively known as a project.
  

    [3]
   

